{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa3a90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_data(dataset_path, num_workers=0, batch_size=128):\n",
    "    data = []\n",
    "    transform = transforms.ToTensor()\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    triangles = [join(dataset_path+\"/triangle/\", f) for f in listdir(dataset_path + \"/triangle/\") if isfile(join(dataset_path+\"/triangle/\", f)) and f.startswith('.')==False]\n",
    "    squares = [join(dataset_path+\"/square/\", f) for f in listdir(dataset_path + \"/square/\") if isfile(join(dataset_path+\"/square/\", f)) and f.startswith('.')==False ] \n",
    "    stars = [join(dataset_path+\"/star/\", f) for f in listdir(dataset_path + \"/star/\") if isfile(join(dataset_path+\"/star/\", f)) and f.startswith('.')==False ]\n",
    "    circles = [join(dataset_path+\"/circle/\", f) for f in listdir(dataset_path + \"/circle/\") if isfile(join(dataset_path+\"/circle/\", f)) and f.startswith('.')==False ]\n",
    "    for i in triangles:\n",
    "        image = Image.open(i)\n",
    "        tup = (transform(image), 0)\n",
    "        data.append(tup)\n",
    "    for i in squares:\n",
    "        image = Image.open(i)\n",
    "        tup = (transform(image), 1)\n",
    "        data.append(tup)\n",
    "    for i in stars:\n",
    "        image = Image.open(i)\n",
    "        tup = (transform(image), 2)\n",
    "        data.append(tup)\n",
    "    for i in circles:\n",
    "        image = Image.open(i)\n",
    "        tup = (transform(image), 3)\n",
    "        data.append(tup)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09d1cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14949, 1, 200, 200])\n",
      "Epoch :  0  Loss :  0.09288325160741806\n",
      "Epoch :  1  Loss :  0.0018254071474075317\n",
      "Epoch :  2  Loss :  0.0\n",
      "Epoch :  3  Loss :  0.06690317392349243\n",
      "Epoch :  4  Loss :  0.2707478404045105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class ClassificationLoss(torch.nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        m = torch.nn.LogSoftmax(dim=1)\n",
    "        loss = torch.nn.NLLLoss()\n",
    "        return loss(m(input), target)\n",
    "\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(1 * 200 * 200, 4)\n",
    "        torch.nn.init.normal_(self.linear1.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.linear1.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x.view((x.size(0),-1)))\n",
    "\n",
    "model_factory = {'linear': LinearClassifier}\n",
    "model = model_factory['linear']()\n",
    "\n",
    "train_data = load_data(\"/Users/shivam/Downloads/archive/shapes/\")\n",
    "ten = []\n",
    "lab = []\n",
    "for p in range(len(train_data)):\n",
    "    i, lb = train_data[p]\n",
    "    lab.append(lb)\n",
    "    ten.append(i)\n",
    "inp = torch.stack(ten, dim=0)\n",
    "print(inp.shape)\n",
    "labels = torch.Tensor(lab).to(torch.int64)\n",
    "\n",
    "\n",
    "nepoch = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9,weight_decay=1e-4)\n",
    "loss = ClassificationLoss()\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    permutation = torch.randperm(len(train_data))\n",
    "    for itr in range(0, len(permutation) - batch_size + 1, batch_size):\n",
    "        batch_samples = permutation[itr:itr + batch_size]\n",
    "        batch_data, batch_label = inp[batch_samples], labels[batch_samples]\n",
    "        loss_val = loss(model(batch_data), batch_label.to(dtype=torch.int64))\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch : ', epoch, ' Loss : ', float(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13f0460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label :  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=200x200 at 0x7FE0E2E5FB80>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2,50,120,200\n",
    "name=\"200.png\"\n",
    "from torchvision import transforms\n",
    "transform = transforms.ToTensor()\n",
    "image = transform(Image.open(\"/Users/shivam/Downloads/archive/shapes/test/\" + name))\n",
    "ten = []\n",
    "ten.append(image)\n",
    "inp = torch.stack(ten, dim=0)\n",
    "# 0 - Triangle\n",
    "# 1 - Square\n",
    "# 2 - Star\n",
    "# 3 - Circle\n",
    "print(\" Label : \", torch.argmax(model(inp)).numpy())\n",
    "Image.open(\"/Users/shivam/Downloads/archive/shapes/test/\" + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32da2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
